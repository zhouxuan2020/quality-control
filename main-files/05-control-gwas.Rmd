
# control GWAS

Here we want to do GWASs that will serve as the control group of the good GWASs. Recall for the bad GWASs (section \@ref(bad)), we have a total of 100k individuals, which consist of 93,528 unrelated white British and 6,472 blacks and Asians. We observed inflation of the test statistics. Although unlikely, it is still possible that some of the inflation is due to random errors. To rule out this possibility [to ascertain that the observed inflation is due to population stratification, not random errors], we used the same 93,528 unrelated white British but replaced the 6,472 blacks and Asians with unrelated white British that are not included in the good GWASs.

## ID list

We used previously obtained id lists (see section \@ref(goodid) for the good GWAS id list and section \@ref(badid) for the bad GWAS id list ) to derive the id list for the control GWASs. These are:

  * `overlap-complete-cases.id`: N = 147,008. Unrelated white British who have no missing data for all 14 traits and covariates.
  * `rand.100000`: ID list for the good gwas. Randomly selected from `overlap-complete-cases.id`  
  * `white.rand.93528`: unrelated whites as a part of the bad gwas ID list. Randomly selected from `rand.100000`
  
Here are the steps:

1. identify IDs from `overlap-complete-cases.id` not included in `rand.100000`.
2. randomly select 6,472 from the identified IDs.
3. combine `white.rand.93528` with the randomly selected 6,472 IDs.  

```{R eval=F}

options(scipen = 100)
id1=read.table("unrelated/overlap-complete-cases.id", header=F)
id2=read.table("unrelated/rand.100000", header=F)
id3=read.table("white.rand.93528", header=F)

# pool for selection
common=intersect(id1$V1, id2$V1)
pool=id1[!id1$V1%in%common,]

# randomly select 6,472 from the pool
sel=pool[sample(dim(pool)[1], 6472, replace=F),]

#  combine with 'white.rand.93528'
write.table(rbind(sel,id3), 'control-gwas.id', quote=F, row.names=F, col.names=F)

```

## QC of hp3 SNPs

```{bash eval=F}

# QC SNPs-----------------------------------------------------------------------
# stating number of SNPs= 1,184,423 
for j in {1..22}; do
echo "#"'!'"/bin/bash
#SBATCH --constraint \"s04|s05\"
#SBATCH --partition normal
#SBATCH --mem 20G
#SBATCH -c 3
#SBATCH -t 1:0:0

./plink2 --pfile ../gen/geno_plink/bhr$j \
         --keep ../control-gwas.id \
         --extract ../doug/ukbb.ldsc \
         --hwe 0.0001 \
         --hard-call-threshold .05 \
         --mach-r2-filter 0.8 2 \
         --make-bed \
         --memory 20000 \
         --out ../gen/tmp/bhr$j \
         --threads 3
"> sh_script/chr$j.sh
done

# submit jobs
for j in {1..22}; do
sbatch -A snpher ../sh_script/chr$j.sh
done > ../job-records/qc-control-gwas

file=job-records/qc-control-gwas
jobs=`awk '{print $4}' $file`
mkdir $file-tmp
for i in $jobs; do
jobinfo $i | awk -F ":" -v i=$i '$1~/Name/ {print i, $2}' >> $file-tmp/name.tmp 
jobinfo $i | awk -F ":" '$1~/State/ {print$2}' >> $file-tmp/state.tmp
jobinfo $i | awk -F ":" '$1~/Cores/ {print$2}' >> $file-tmp/cores.tmp
jobinfo $i | awk -F ":" '$1~/Used walltime/ {print $2 ":" $3 ":" $4}' >> $file-tmp/time.tmp
jobinfo $i | awk -F ":" '$1~/Max Mem/ {split($2,a,/[(]/ ); print a[1]}' >> $file-tmp/mem.tmp
done
paste $file-tmp/name.tmp \
      $file-tmp/state.tmp \
      $file-tmp/cores.tmp \
      $file-tmp/time.tmp \
      $file-tmp/mem.tmp \
      | awk 'BEGIN{print "ID name state cores time mem"}{print $0}' > $file.out
rm -r $file-tmp

# merge files
rm bfile-control-gwas.list
for j in {1..22}; do
echo  "../gen/tmp/bhr$j" >>bfile-control-gwas.list
done

echo "#"'!'"/bin/bash
#SBATCH --constraint \"s04|s05\"
#SBATCH --partition normal
#SBATCH --mem 20G
#SBATCH -c 3
#SBATCH -t 40:0:0
./ldak5.1 --make-bed ../gen/geno-control \
          --mbfile ../gen/bfile-control-gwas.list \
          --max-threads 3 \
          --exclude-dups YES  
"> sh_script/mbfile-control.sh

# submit the script
sbatch -A snpher ../sh_script/mbfile-control.sh >../job-records/mbfiles-control-gwas

# MAF & call-rate 
awk < geno-control.stats '($5>.01 && $6>=0.95 && NR>1){print $1}' > snps-control-gwas.use
# m = 1,103,182 SNPs

```

## GWAS

```{bash eval=F}

# linear regression-------------------------------------------------------------
mkdir gwas-control

dirout=gwas-control
filein=geno-control
id=control-gwas.id
snp=snps-control-gwas.use
for i in {awake,bmi,chron,ever,fvc,height,imp,neur,pulse,quals,reaction,sbp,snoring,hyper}; do
echo "#"'!'"/bin/bash
#SBATCH --constraint \"s04|s05\"
#SBATCH --partition normal
#SBATCH --mem 10G
#SBATCH -c 2
#SBATCH -t 10:0:0

./ldak5.1 --linear ../$dirout/$i-linear \
          --pheno ../phen/continuous-traits/$i.raw.pheno \
          --bfile ../gen/$filein \
          --keep ../$id \
          --extract ../gen/$snp \
          --covar ../phen/covariates.use \
          --max-threads 2 \
"> sh_script/$i-linear.sh
done

# submit files------------------------------------------------------------------
for i in {awake,bmi,chron,ever,fvc,height,imp,neur,pulse,quals,reaction,sbp,snoring,hyper}; do
sbatch -A snpher ../sh_script/$i-linear.sh
done>../job-records/gwas-control

# check job completion----------------------------------------------------------
file=job-records/gwas-control
jobs=`awk '{print $4}' $file`
mkdir $file-tmp
for i in $jobs; do
jobinfo $i | awk -F ":" -v i=$i '$1~/Name/ {print i, $2}' >> $file-tmp/name.tmp 
jobinfo $i | awk -F ":" '$1~/State/ {print$2}' >> $file-tmp/state.tmp
jobinfo $i | awk -F ":" '$1~/Cores/ {print$2}' >> $file-tmp/cores.tmp
jobinfo $i | awk -F ":" '$1~/Used walltime/ {print $2 ":" $3 ":" $4}' >> $file-tmp/time.tmp
jobinfo $i | awk -F ":" '$1~/Max Mem/ {split($2,a,/[(]/ ); print a[1]}' >> $file-tmp/mem.tmp
done
paste $file-tmp/name.tmp \
      $file-tmp/state.tmp \
      $file-tmp/cores.tmp \
      $file-tmp/time.tmp \
      $file-tmp/mem.tmp \
      | awk 'BEGIN{print "ID name state cores time mem"}{print $0}' > $file.out
rm -r $file-tmp

```

## ldsc intercept

### under gcta

```{bash eval=F}
# UP TO HERE

#:::::::::::::
# using original ldsc
#:::::::::::::

# format stats for ldsc--------------------------------------------------------- 
library(vroom)
options(scipen = 100)
rs=vroom("doug/ukbb.ldsc", col_names=F)
phen=c("awake","bmi","chron","ever","fvc",
       "height","imp","neur","pulse","quals",
       "reaction","sbp","snoring","hyper", "quals")

for(i in phen){
  stat=vroom(paste0("gwas-norm/",i,"-linear.summaries"), col_names=T)
  p=vroom(paste0("gwas-norm/",i,"-linear.pvalues"), col_names=T)
  m1=match(rs$X1, stat$Predictor)
  m2=match(rs$X1, p$Predictor)
  out=data.frame(SNP=rs$X2,
                 N=stat$n[m1],
                 Z=(sqrt(stat$Stat)*stat$Direction)[m1],
                 A1=stat$A1[m1],
                 A2=stat$A2[m1], 
                 pval=p$P[m2], stringsAsFactors=F)
  out=out[complete.cases(out),]
  write.table(out, paste0("gwas-norm/",i,"-linear-rs.summaries"), 
  col.names=T, row.names=F, quote=F)
}

# perform ldsc------------------------------------------------------------------
mkdir out-norm-100k
for i in {awake,bmi,chron,ever,fvc,height,quals,imp,neur,pulse,reaction,sbp,snoring,hyper}; do
echo "#"'!'"/bin/bash
#SBATCH --constraint \"s04|s05\"
#SBATCH --partition normal
#SBATCH --mem 2G
#SBATCH -c 3
#SBATCH -t 24:00:0

.././munge_sumstats.py \
--sumstats ../../gwas-norm/$i-linear-rs.summaries \
--out ../out-norm-100k/$i \
--merge-alleles ../w_hm3.snplist

.././ldsc.py \
--h2 ../out-norm-100k/$i.sumstats.gz \
--ref-ld-chr ../eur_w_ld_chr/ \
--w-ld-chr ../eur_w_ld_chr/ \
--out ../out-norm-100k/$i-ldsc
">sh_script/ldsc-$i-norm-100k.sh
done

# submit jobs-------------------------------------------------------------------
for i in {awake,bmi,chron,ever,fvc,height,imp,neur,pulse,reaction,sbp,snoring,hyper,quals}; do
sbatch -A snpher ../sh_script/ldsc-$i-norm-100k.sh
done>../../job-records/ldsc-norm-100k

#:::::::::::::
# using sumher
#:::::::::::::

# insert genetic distance into bim file-----------------------------------------
head geno-norm-100k.fam > small-norm

for j in {1..22}; do
echo "#"'!'"/bin/bash
#SBATCH --mem 8G
#SBATCH -c 1
#SBATCH -t 3:00:0
#SBATCH --constraint \"s04|s05\"
./plink1.9 --bfile ../gen/geno-norm-100k \
          --chr $j \
          --cm-map /home/zhoux/snpher/faststorage/genetic_maps/genetic_map_chr@_combined_b37.txt \
          --make-bed \
          --out new$j \
          --keep ../gen/small-norm
" > sh_script/map$j
done

for j in {1..22}; do
sbatch -A snpher ../sh_script/map$j
done > genetic-distance-hapmap3

cd /home/zhoux/dsmwpred/xuan/quality-control/qc-10oct/gen
cat new{1..22}.bim | awk '{print $2, $3}' > maps-hapmap3-norm-100k.txt
rm new{1..22}.{bim,bed,fam,log}

awk '(NR==FNR){arr[$1]=$2;next}{print $1, $2, arr[$2], $4, $5, $6}' > geno-norm-100k.bim2 maps-hapmap3-norm-100k.txt geno-norm-100k.bim
mv geno-norm-100k.bim geno-norm-100k.bim0
mv geno-norm-100k.bim2 geno-norm-100k.bim

# compute tagging under gcta----------------------------------------------------
mkdir tagging-norm-100k
for j in {1..22}; do
echo "#"'!'"/bin/bash
#SBATCH --constraint \"s04|s05\"
#SBATCH --partition normal
#SBATCH --mem 5G
#SBATCH -c 1
#SBATCH -t 5:0:0

./ldak5.1 --calc-tagging ../tagging-norm-100k/gcta-hapmap3-chr-$j \
          --bfile ../gen/geno-norm-100k \
          --ignore-weights YES \
          --power -1 \
          --window-cm 1 \
          --chr $j
"> sh_script/tagging-gcta-hapmap3-chr$j
done

for j in {1..22}; do
sbatch -A snpher ../sh_script/tagging-gcta-hapmap3-chr$j 
done > ../job-records/tagging-gcta-hapmap3-norm-100k

# check job completion----------------------------------------------------------
file=job-records/tagging-gcta-hapmap3-norm-100k
jobs=`awk '{print $4}' $file`
mkdir $file-tmp
for i in $jobs; do
jobinfo $i | awk -F ":" -v i=$i '$1~/Name/ {print i, $2}' >> $file-tmp/name.tmp 
jobinfo $i | awk -F ":" '$1~/State/ {print$2}' >> $file-tmp/state.tmp
jobinfo $i | awk -F ":" '$1~/Cores/ {print$2}' >> $file-tmp/cores.tmp
jobinfo $i | awk -F ":" '$1~/Used walltime/ {print $2 ":" $3 ":" $4}' >> $file-tmp/time.tmp
jobinfo $i | awk -F ":" '$1~/Max Mem/ {split($2,a,/[(]/ ); print a[1]}' >> $file-tmp/mem.tmp
done
paste $file-tmp/name.tmp \
      $file-tmp/state.tmp \
      $file-tmp/cores.tmp \
      $file-tmp/time.tmp \
      $file-tmp/mem.tmp \
      | awk 'BEGIN{print "ID name state cores time mem"}{print $0}' > $file.out
rm -r $file-tmp

# merge tagging files-----------------------------------------------------------
rm list.txt
for j in {1..22}; do 
echo "tagging-norm-100k/gcta-hapmap3-chr-$j.tagging" >> list.txt
done
./ldak5.1 --join-tagging tagging-norm-100k/gcta-hapmap3 --taglist list.txt

# ldsc intercept----------------------------------------------------------------
mkdir sumher-norm-100k
dirin1=tagging-norm-100k
dirin2=gwas-norm
dirout=sumher-norm-100k
for i in {awake,bmi,chron,ever,fvc,height,quals,imp,neur,pulse,reaction,sbp,snoring,hyper}; do
echo "#"'!'"/bin/bash
#SBATCH --constraint \"s04|s05\"
#SBATCH --partition normal
#SBATCH --mem 5G
#SBATCH -c 1
#SBATCH -t 5:0:0

./ldak5.1 --sum-hers ../$dirout/$i-sumher-gcta \
          --tagfile ../$dirin1/gcta-hapmap3.tagging \
          --summary ../$dirin2/$i-linear.summaries \
          --check-sums NO \
          --intercept YES
"> sh_script/$i-sumher-gcta
done

for i in {awake,bmi,chron,ever,fvc,height,quals,imp,neur,pulse,reaction,sbp,snoring,hyper}; do
sbatch -A snpher ../sh_script/$i-sumher-gcta
done>../job-records/sumher-gcta-norm-100k

```

### under ldak-thin

```{bash eval=F}

# get weights-------------------------------------------------------------------
awk < ldak-thin/ldak-thin-norm.in '{print $1, 1}' > ldak-thin/weights.ldak-thin-norm

# calculate tagging under ldak-thin---------------------------------------------
dirout=tagging-norm-100k
filein1=geno-norm-100k
filein2=weights.ldak-thin-norm
for j in {1..22}; do
echo "#"'!'"/bin/bash
#SBATCH --partition normal
#SBATCH --mem 40G
#SBATCH -c 5
#SBATCH -t 10:0:0
#SBATCH --constraint \"s04|s05\"
./ldak5.1 --calc-tagging ../$dirout/ldak-thin-hapmap3-chr-$j \
          --bfile ../gen/$filein1 \
          --weights ../ldak-thin/$filein2 \
          --power -.25 \
          --window-cm 1 \
          --chr $j \
          --save-matrix YES \
          --max-threads 5
" > sh_script/tagging-ldak-thin-hapmap3-chr$j.sh
done

# submit scripts
for j in {1..22}; do
sbatch -A snpher ../sh_script/tagging-ldak-thin-hapmap3-chr$j.sh
done > ../job-records/tagging-ldak-thin-norm-100k

# check job completion----------------------------------------------------------
file=job-records/tagging-ldak-thin-norm-100k
jobs=`awk '{print $4}' $file`
mkdir $file-tmp
for i in $jobs; do
jobinfo $i | awk -F ":" -v i=$i '$1~/Name/ {print i, $2}' >> $file-tmp/name.tmp 
jobinfo $i | awk -F ":" '$1~/State/ {print$2}' >> $file-tmp/state.tmp
jobinfo $i | awk -F ":" '$1~/Cores/ {print$2}' >> $file-tmp/cores.tmp
jobinfo $i | awk -F ":" '$1~/Used walltime/ {print $2 ":" $3 ":" $4}' >> $file-tmp/time.tmp
jobinfo $i | awk -F ":" '$1~/Max Mem/ {split($2,a,/[(]/ ); print a[1]}' >> $file-tmp/mem.tmp
done
paste $file-tmp/name.tmp \
      $file-tmp/state.tmp \
      $file-tmp/cores.tmp \
      $file-tmp/time.tmp \
      $file-tmp/mem.tmp \
      | awk 'BEGIN{print "ID name state cores time mem"}{print $0}' > $file.out
rm -r $file-tmp

# merge tagging files-----------------------------------------------------------
rm list.txt
rm matlist.txt
for j in {1..22}; do
echo "tagging-norm-100k/ldak-thin-hapmap3-chr-$j.tagging" >> list.txt
echo "tagging-norm-100k/ldak-thin-hapmap3-chr-$j.matrix" >> matlist.txt
done
dirout=tagging-norm-100k
./ldak5.1 --join-tagging $dirout/ldak-thin-hapmap3 --taglist list.txt --matlist matlist.txt

# ldsc intercept----------------------------------------------------------------
dirin1=tagging-norm-100k
dirin2=gwas-norm
dirout=sumher-norm-100k
for i in {awake,bmi,chron,ever,fvc,height,quals,imp,neur,pulse,reaction,sbp,snoring,hyper}; do
echo "#"'!'"/bin/bash
#SBATCH --constraint \"s04|s05\"
#SBATCH --partition normal
#SBATCH --mem 5G
#SBATCH -c 1
#SBATCH -t 00:30:0

./ldak5.1 --sum-hers ../$dirout/$i-sumher-ldak-thin \
          --tagfile ../$dirin1/ldak-thin-hapmap3.tagging \
          --summary ../$dirin2/$i-linear.summaries \
          --check-sums NO \
          --intercept YES
"> sh_script/$i-sumher-ldak-thin
done

for i in {awake,bmi,chron,ever,fvc,height,quals,imp,neur,pulse,reaction,sbp,snoring,hyper}; do
sbatch -A snpher ../sh_script/$i-sumher-ldak-thin
done>../job-records/sumher-ldak-thin-norm-100k

```

### summary

```{bash eval=F}

# original ldsc
grep Intercept *ldsc.log | awk '{split($1, a, /[-]/); split($3, b, /[()]/); print a[1], $2, b[2]}' > ../../summary/ldsc-norm-100k

# sumher under gcta
grep Intercept_Estimate *gcta.extra | awk '{ split($1, a, /[-]/); print a[1], $2}' >../summary/sumher-gcta-norm-100k-est
grep Intercept_SD *gcta.extra | awk '{ split($1, a, /[-]/); print a[1], $2}' >../summary/sumher-gcta-norm-100k-sd
cd ../summary/
paste sumher-gcta-norm-100k-est sumher-gcta-norm-100k-sd | awk '{print $1, $2, $4}' > sumher-gcta-norm-100k
      
# suhmer under ldak-thin
grep Intercept_Estimate *ldak-thin.extra | awk '{ split($1, a, /[-]/); print a[1], $2}' >../summary/sumher-ldak-thin-norm-100k-est
grep Intercept_SD *ldak-thin.extra | awk '{ split($1, a, /[-]/); print a[1], $2}' >../summary/sumher-ldak-thin-norm-100k-sd
cd ../summary/
paste sumher-ldak-thin-norm-100k-est sumher-ldak-thin-norm-100k-sd | awk '{print $1, $2, $4}' > sumher-ldak-thin-norm-100k

```


